# -*- coding: utf-8 -*-
"""SSD_MobileNet_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VWvO_euezZhetyGUJPdDa-dv7QRFwPND
"""

import torch
import torch.nn as nn
import torchvision
import math
import numpy as np
import cv2
import os
import xml.etree.ElementTree as ET
from torch.utils.data import Dataset, DataLoader
from torchvision import tv_tensors
from torchvision.io import read_image
import torchvision.transforms.v2 as T
from tqdm import tqdm
import random
import argparse
from torchsummary import summary
from sklearn.metrics import precision_score, recall_score, f1_score
from collections import defaultdict

# [Previous configuration code remains exactly the same...]
# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.backends.mps.is_available():
    device = torch.device('mps')
    print('Using mps')

dir = "/mnt/d/NIT-Notes/SEM-3/practical/SSD/leaf_SSD"
# Configuration (simplified from your YAML)
config = {
    'dataset_params': {
        'train_im_sets': [dir +'/train'],
        'test_im_sets': [dir +'/test'],
        'num_classes': 28,
        'im_size': 300
    },
    'model_params': {
        'aspect_ratios': [
            [1., 2., 0.5],
            [1., 2., 3., 0.5, .333],
            [1., 2., 3., 0.5, .333],
            [1., 2., 3., 0.5, .333],
            [1., 2., 0.5],
            [1., 2., 0.5]
        ],
        'scales': [0.1, 0.2, 0.375, 0.55, 0.725, 0.9],
        'iou_threshold': 0.5,
        'low_score_threshold': 0.01,
        'neg_pos_ratio': 3,
        'pre_nms_topK': 400,
        'detections_per_img': 28,
        'nms_threshold': 0.3
    },
    'train_params': {
        'task_name': 'ssd_custom',
        'seed': 1111,
        'acc_steps': 1,
        'num_epochs': 32,
        'batch_size': 32,
        'lr_steps': [40, 50, 60, 70, 80, 90],
        'lr': 0.001,
        'log_steps': 100,
        'infer_conf_threshold': 0.8,
        'ckpt_name': 'ssd_custom.pth'
    }
}

# Dataset Class
class PlantDiseaseDataset(Dataset):
    def __init__(self, split, im_sets, im_size=300):
        self.split = split
        self.im_sets = im_sets
        self.im_size = im_size

        # Classes
        self.classes = ['background','Cassava Bacterial Blight',
             'Cassava Brown Leaf Spot','Cassava Healthy','Cassava Mosaic',
             'Cassava Root Rot','Corn Brown Spots','Corn Charcoal',
             'Corn Chlorotic Leaf Spot','Corn Gray leaf spot','Corn Healthy',
             'Corn Insects Damages','Corn leaf blight','Corn Mildew',
             'Corn Purple Discoloration','Corn rust leaf','Corn Smut',
             'Corn Streak','Corn Stripe','Corn Violet Decoloration',
             'Corn Yellow Spots','Corn Yellowing','Tomato bacterial wilt',
             'Tomato blight leaf','Tomato Brown Spots','Tomato healthy',
             'Tomato leaf mosaic virus','Tomato leaf yellow virus']
        self.label2idx = {cls: idx for idx, cls in enumerate(self.classes)}
        self.idx2label = {idx: cls for idx, cls in enumerate(self.classes)}

        # Image normalization
        self.im_mean = [123.0, 117.0, 104.0]
        self.imagenet_mean = [0.485, 0.456, 0.406]
        self.imagenet_std = [0.229, 0.224, 0.225]

        # Load data
        self.images_info = self.load_images_and_anns()
        self.transforms = self.get_transforms()

    def get_transforms(self):
        if self.split == 'train':
            return T.Compose([
                T.RandomPhotometricDistort(),
                T.RandomZoomOut(fill=list(self.im_mean)),
                T.RandomIoUCrop(),
                T.RandomHorizontalFlip(p=0.5),
                T.Resize((self.im_size, self.im_size)),
                T.SanitizeBoundingBoxes(),
                T.ToPureTensor(),
                T.ToDtype(torch.float32, scale=True),
                T.Normalize(mean=self.imagenet_mean, std=self.imagenet_std)
            ])
        else:
            return T.Compose([
                T.Resize((self.im_size, self.im_size)),
                T.ToPureTensor(),
                T.ToDtype(torch.float32, scale=True),
                T.Normalize(mean=self.imagenet_mean, std=self.imagenet_std)
            ])

    def load_images_and_anns(self):
        im_infos = []
        for im_set in self.im_sets:
            xml_dir = os.path.join(im_set, 'XML')
            img_dir = os.path.join(im_set, 'images')

            for xml_file in os.listdir(xml_dir):
                if not xml_file.endswith('.xml'):
                    continue

                base_name = os.path.splitext(xml_file)[0]
                img_path = os.path.join(img_dir, f"{base_name}.jpg")

                if not os.path.exists(img_path):
                    continue

                tree = ET.parse(os.path.join(xml_dir, xml_file))
                root = tree.getroot()

                size = root.find('size')
                im_info = {
                    'filename': img_path,
                    'width': int(size.find('width').text),
                    'height': int(size.find('height').text),
                    'detections': []
                }

                for obj in root.findall('object'):
                    bbox = obj.find('bndbox')
                    im_info['detections'].append({
                        'label': self.label2idx[obj.find('name').text.strip()],
                        'difficult': int(obj.find('difficult').text),
                        'bbox': [
                            int(float(bbox.find('xmin').text)) - 1,
                            int(float(bbox.find('ymin').text)) - 1,
                            int(float(bbox.find('xmax').text)) - 1,
                            int(float(bbox.find('ymax').text)) - 1
                        ]
                    })

                im_infos.append(im_info)

        print(f'Loaded {len(im_infos)} {self.split} images')
        return im_infos

    def __len__(self):
        return len(self.images_info)

    def __getitem__(self, idx):
        im_info = self.images_info[idx]
        image = read_image(im_info['filename'])

        if len(im_info['detections']) == 0:
            dummy_box = torch.tensor([[0, 0, 1, 1]], dtype=torch.float32)
            dummy_label = torch.tensor([0], dtype=torch.int64)
            dummy_difficult = torch.tensor([0], dtype=torch.int64)

            target = {
                'boxes': tv_tensors.BoundingBoxes(dummy_box, format='XYXY', canvas_size=image.shape[-2:]),
                'labels': dummy_label,
                'difficult': dummy_difficult
            }

            image, target = self.transforms(image, target)
            h, w = image.shape[-2:]
            target['boxes'][:, [0, 2]] /= w
            target['boxes'][:, [1, 3]] /= h

            return image, target, im_info['filename']

        boxes = [det['bbox'] for det in im_info['detections']]
        labels = [det['label'] for det in im_info['detections']]
        difficult = [det['difficult'] for det in im_info['detections']]

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        difficult = torch.as_tensor(difficult, dtype=torch.int64)

        boxes = tv_tensors.BoundingBoxes(boxes, format='XYXY', canvas_size=image.shape[-2:])

        target = {
            'boxes': boxes,
            'labels': labels,
            'difficult': difficult
        }

        image, target = self.transforms(image, target)

        h, w = image.shape[-2:]
        target['boxes'][:, [0, 2]] /= w
        target['boxes'][:, [1, 3]] /= h

        return image, target, im_info['filename']

# SSD Model

class SSD(nn.Module):
    def __init__(self, config, num_classes=28):
        super().__init__()
        self.aspect_ratios = config['aspect_ratios']
        self.scales = config['scales']
        self.scales.append(1.0)
        self.num_classes = num_classes
        self.iou_threshold = config['iou_threshold']
        self.low_score_threshold = config['low_score_threshold']
        self.neg_pos_ratio = config['neg_pos_ratio']
        self.pre_nms_topK = config['pre_nms_topK']
        self.nms_threshold = config['nms_threshold']
        self.detections_per_img = config['detections_per_img']

        # Backbone (MobileNetV2)
        backbone = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1)
        self.features = backbone.features

        # We'll use these layers from MobileNetV2 as feature maps:
        # - layer 3 (stride 8) - output channels: 24
        # - layer 6 (stride 16) - output channels: 32
        # - layer 13 (stride 32) - output channels: 96
        # - final layer (stride 32) - output channels: 1280

        # Additional layers for SSD (adjusted to prevent size reduction to 1x1)
        self.extra_layers = nn.ModuleList([
            # Additional layer after MobileNetV2 features (output: stride 32)
            nn.Sequential(
                nn.Conv2d(1280, 512, kernel_size=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=2),
                nn.ReLU(inplace=True)
            ),

            # Additional layer (output: stride 64)
            nn.Sequential(
                nn.Conv2d(512, 256, kernel_size=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2),
                nn.ReLU(inplace=True)
            )
        ])

        # Scale weight for first feature map (layer 3 output)
        self.scale_weight = nn.Parameter(torch.ones(24) * 20)

        # Output channels for each feature map
        out_channels = [24, 32, 96, 1280, 512, 256]

        # Prediction heads
        self.cls_heads = nn.ModuleList()
        for channels, aspect_ratio in zip(out_channels, self.aspect_ratios):
            num_anchors = len(aspect_ratio) + 1  # +1 for default aspect ratio
            self.cls_heads.append(nn.Conv2d(channels, num_anchors * self.num_classes,
                                 kernel_size=3, padding=1))

        self.bbox_reg_heads = nn.ModuleList()
        for channels, aspect_ratio in zip(out_channels, self.aspect_ratios):
            num_anchors = len(aspect_ratio) + 1
            self.bbox_reg_heads.append(nn.Conv2d(channels, num_anchors * 4,
                                      kernel_size=3, padding=1))

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        for layer in self.extra_layers.modules():
            if isinstance(layer, nn.Conv2d):
                nn.init.xavier_uniform_(layer.weight)
                if layer.bias is not None:
                    nn.init.constant_(layer.bias, 0.0)

        for module in self.cls_heads:
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)
        for module in self.bbox_reg_heads:
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0.0)

    def forward(self, x, targets=None):
        # Feature extraction from MobileNetV2
        features = []

        # Forward pass through MobileNetV2 features
        for i, layer in enumerate(self.features):
            x = layer(x)
            # Capture feature maps at specific layers
            if i == 3:  # First expansion layer output (stride 8)
                features.append(x)
            elif i == 6:  # Second expansion layer output (stride 16)
                features.append(x)
            elif i == 13:  # Final MobileNetV2 features (stride 32)
                features.append(x)

        # Get the final MobileNetV2 features (stride 32)
        features.append(x)

        # Forward pass through extra layers
        x = features[-1]
        for layer in self.extra_layers:
            x = layer(x)
            features.append(x)

        # Apply scale weight to the first feature map
        features[0] = (self.scale_weight.view(1, -1, 1, 1) *
                      torch.nn.functional.normalize(features[0]))

        # Predictions
        cls_logits = []
        bbox_reg_deltas = []
        for i, feature_map in enumerate(features):
            cls_feat_i = self.cls_heads[i](feature_map)
            bbox_reg_feat_i = self.bbox_reg_heads[i](feature_map)

            N, _, H, W = cls_feat_i.shape
            cls_feat_i = cls_feat_i.view(N, -1, self.num_classes, H, W)
            cls_feat_i = cls_feat_i.permute(0, 3, 4, 1, 2).reshape(N, -1, self.num_classes)
            cls_logits.append(cls_feat_i)

            N, _, H, W = bbox_reg_feat_i.shape
            bbox_reg_feat_i = bbox_reg_feat_i.view(N, -1, 4, H, W)
            bbox_reg_feat_i = bbox_reg_feat_i.permute(0, 3, 4, 1, 2).reshape(N, -1, 4)
            bbox_reg_deltas.append(bbox_reg_feat_i)

        cls_logits = torch.cat(cls_logits, dim=1)
        bbox_reg_deltas = torch.cat(bbox_reg_deltas, dim=1)

        # Generate default boxes
        default_boxes = self.generate_default_boxes(features)

        if self.training:
            return self.compute_loss(targets, cls_logits, bbox_reg_deltas, default_boxes)
        else:
            return self.postprocess_detections(cls_logits, bbox_reg_deltas, default_boxes)

    def summary(self):
        """Print model architecture summary"""
        sample_input = torch.randn(1, 3, config['dataset_params']['im_size'],
                                 config['dataset_params']['im_size']).to(device)
        print("\nModel Architecture Summary:")
        print(f"Input shape: {sample_input.shape}")

        # Print backbone summary
        print("\nMobileNetV2 Backbone:")
        print(self.features)

        # Print extra layers
        print("\nAdditional Layers:")
        for i, layer in enumerate(self.extra_layers):
            print(f"Extra Layer {i+1}:")
            print(layer)

        # Print prediction heads
        print("\nPrediction Heads:")
        print("Classification Heads:", self.cls_heads)
        print("Bounding Box Heads:", self.bbox_reg_heads)

        # Calculate and print parameters
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"\nTotal Parameters: {total_params:,}")
        print(f"Trainable Parameters: {trainable_params:,}")

        # Print feature map sizes
        print("\nFeature Map Sizes:")
        x = sample_input
        print(f"Input size: {x.shape}")
        for i, layer in enumerate(self.features):
            x = layer(x)
            if i in [3, 6, 13]:  # Important layers we're using
                print(f"Layer {i} output: {x.shape}")

        # Final features
        print(f"Final features output: {x.shape}")

        for i, layer in enumerate(self.extra_layers):
            x = layer(x)
            print(f"Extra Layer {i+1} output: {x.shape}")

    # [Rest of the methods remain the same as in your original SSD implementation...]
    def generate_default_boxes(self, feat):
        default_boxes = []
        for k in range(len(feat)):
            s_prime_k = math.sqrt(self.scales[k] * self.scales[k + 1])
            wh_pairs = [[s_prime_k, s_prime_k]]

            for ar in self.aspect_ratios[k]:
                sq_ar = math.sqrt(ar)
                w = self.scales[k] * sq_ar
                h = self.scales[k] / sq_ar
                wh_pairs.extend([[w, h]])

            feat_h, feat_w = feat[k].shape[-2:]
            shifts_x = ((torch.arange(0, feat_w) + 0.5) / feat_w).to(torch.float32)
            shifts_y = ((torch.arange(0, feat_h) + 0.5) / feat_h).to(torch.float32)
            shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x, indexing="ij")
            shift_x = shift_x.reshape(-1)
            shift_y = shift_y.reshape(-1)

            shifts = torch.stack((shift_x, shift_y) * len(wh_pairs), dim=-1).reshape(-1, 2)
            wh_pairs = torch.as_tensor(wh_pairs)
            wh_pairs = wh_pairs.repeat((feat_h * feat_w), 1)
            default_box = torch.cat((shifts, wh_pairs), dim=1)
            default_boxes.append(default_box)

        default_boxes = torch.cat(default_boxes, dim=0)
        dboxes = []
        for _ in range(feat[0].size(0)):
            dboxes_in_image = default_boxes
            dboxes_in_image = torch.cat(
                [(dboxes_in_image[:, :2] - 0.5 * dboxes_in_image[:, 2:]),
                 (dboxes_in_image[:, :2] + 0.5 * dboxes_in_image[:, 2:])], -1)
            dboxes.append(dboxes_in_image.to(feat[0].device))
        return dboxes

    def compute_loss(self, targets, cls_logits, bbox_regression, default_boxes):
        matched_idxs = []
        for default_boxes_per_image, targets_per_image in zip(default_boxes, targets):
            if targets_per_image["boxes"].numel() == 0:
                matched_idxs.append(torch.full((default_boxes_per_image.size(0),), -1,
                                             dtype=torch.int64, device=default_boxes_per_image.device))
                continue

            iou_matrix = self.get_iou(targets_per_image["boxes"], default_boxes_per_image)
            matched_vals, matches = iou_matrix.max(dim=0)
            matches[matched_vals < self.iou_threshold] = -1

            _, highest_quality_pred_foreach_gt = iou_matrix.max(dim=1)
            matches[highest_quality_pred_foreach_gt] = torch.arange(
                highest_quality_pred_foreach_gt.size(0), dtype=torch.int64,
                device=highest_quality_pred_foreach_gt.device)
            matched_idxs.append(matches)

        num_foreground = 0
        bbox_loss = []
        cls_targets = []

        for (targets_per_image, bbox_regression_per_image,
             cls_logits_per_image, default_boxes_per_image, matched_idxs_per_image) in zip(
                targets, bbox_regression, cls_logits, default_boxes, matched_idxs):

            fg_idxs_per_image = torch.where(matched_idxs_per_image >= 0)[0]
            foreground_matched_idxs_per_image = matched_idxs_per_image[fg_idxs_per_image]
            num_foreground += foreground_matched_idxs_per_image.numel()

            matched_gt_boxes_per_image = targets_per_image["boxes"][foreground_matched_idxs_per_image]
            bbox_regression_per_image = bbox_regression_per_image[fg_idxs_per_image, :]
            default_boxes_per_image = default_boxes_per_image[fg_idxs_per_image, :]

            target_regression = self.boxes_to_transformation_targets(
                matched_gt_boxes_per_image, default_boxes_per_image)

            bbox_loss.append(torch.nn.functional.smooth_l1_loss(
                bbox_regression_per_image, target_regression, reduction='sum'))

            gt_classes_target = torch.zeros(
                (cls_logits_per_image.size(0),),
                dtype=targets_per_image["labels"].dtype,
                device=targets_per_image["labels"].device)
            gt_classes_target[fg_idxs_per_image] = targets_per_image["labels"][
                foreground_matched_idxs_per_image]
            cls_targets.append(gt_classes_target)

        bbox_loss = torch.stack(bbox_loss)
        cls_targets = torch.stack(cls_targets)
        num_classes = cls_logits.size(-1)

        cls_loss = torch.nn.functional.cross_entropy(
            cls_logits.view(-1, num_classes), cls_targets.view(-1), reduction="none").view(cls_targets.size())

        foreground_idxs = cls_targets > 0
        num_negative = self.neg_pos_ratio * foreground_idxs.sum(1, keepdim=True)
        negative_loss = cls_loss.clone()
        negative_loss[foreground_idxs] = -float("inf")
        values, idx = negative_loss.sort(1, descending=True)
        background_idxs = idx.sort(1)[1] < num_negative

        N = max(1, num_foreground)
        return {
            "bbox_regression": bbox_loss.sum() / N,
            "classification": (cls_loss[foreground_idxs].sum() +
                              cls_loss[background_idxs].sum()) / N,
        }, None

    def postprocess_detections(self, cls_logits, bbox_reg_deltas, default_boxes):
        cls_scores = torch.nn.functional.softmax(cls_logits, dim=-1)
        num_classes = cls_scores.size(-1)

        detections = []
        for bbox_deltas_i, cls_scores_i, default_boxes_i in zip(bbox_reg_deltas, cls_scores, default_boxes):
            boxes = self.apply_regression_pred_to_default_boxes(bbox_deltas_i, default_boxes_i)
            boxes.clamp_(min=0., max=1.)

            pred_boxes = []
            pred_scores = []
            pred_labels = []

            for label in range(1, num_classes):
                score = cls_scores_i[:, label]
                keep_idxs = score > self.low_score_threshold
                score = score[keep_idxs]
                box = boxes[keep_idxs]

                score, top_k_idxs = score.topk(min(self.pre_nms_topK, len(score)))
                box = box[top_k_idxs]

                pred_boxes.append(box)
                pred_scores.append(score)
                pred_labels.append(torch.full_like(score, fill_value=label,
                                                 dtype=torch.int64, device=cls_scores.device))

            if len(pred_boxes) == 0:
                detections.append({
                    "boxes": torch.empty((0, 4), device=boxes.device),
                    "scores": torch.empty((0,), device=boxes.device),
                    "labels": torch.empty((0,), dtype=torch.int64, device=boxes.device)
                })
                continue

            pred_boxes = torch.cat(pred_boxes, dim=0)
            pred_scores = torch.cat(pred_scores, dim=0)
            pred_labels = torch.cat(pred_labels, dim=0)

            keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)
            for class_id in torch.unique(pred_labels):
                curr_indices = torch.where(pred_labels == class_id)[0]
                curr_keep_idxs = torch.ops.torchvision.nms(
                    pred_boxes[curr_indices], pred_scores[curr_indices], self.nms_threshold)
                keep_mask[curr_indices[curr_keep_idxs]] = True

            keep_indices = torch.where(keep_mask)[0]
            post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]
            keep = post_nms_keep_indices[:self.detections_per_img]

            detections.append({
                "boxes": pred_boxes[keep],
                "scores": pred_scores[keep],
                "labels": pred_labels[keep],
            })

        return None, detections

    @staticmethod
    def get_iou(boxes1, boxes2):
        area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
        area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])

        x_left = torch.max(boxes1[:, None, 0], boxes2[:, 0])
        y_top = torch.max(boxes1[:, None, 1], boxes2[:, 1])
        x_right = torch.min(boxes1[:, None, 2], boxes2[:, 2])
        y_bottom = torch.min(boxes1[:, None, 3], boxes2[:, 3])

        intersection_area = ((x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0))
        union = area1[:, None] + area2 - intersection_area
        iou = intersection_area / union
        return iou

    @staticmethod
    def boxes_to_transformation_targets(ground_truth_boxes, default_boxes, weights=(10., 10., 5., 5.)):
        widths = default_boxes[:, 2] - default_boxes[:, 0]
        heights = default_boxes[:, 3] - default_boxes[:, 1]
        center_x = default_boxes[:, 0] + 0.5 * widths
        center_y = default_boxes[:, 1] + 0.5 * heights

        gt_widths = (ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0])
        gt_heights = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]
        gt_center_x = ground_truth_boxes[:, 0] + 0.5 * gt_widths
        gt_center_y = ground_truth_boxes[:, 1] + 0.5 * gt_heights

        targets_dx = weights[0] * (gt_center_x - center_x) / widths
        targets_dy = weights[1] * (gt_center_y - center_y) / heights
        targets_dw = weights[2] * torch.log(gt_widths / widths)
        targets_dh = weights[3] * torch.log(gt_heights / heights)

        return torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=1)

    @staticmethod
    def apply_regression_pred_to_default_boxes(box_transform_pred, default_boxes, weights=(10., 10., 5., 5.)):
        w = default_boxes[:, 2] - default_boxes[:, 0]
        h = default_boxes[:, 3] - default_boxes[:, 1]
        center_x = default_boxes[:, 0] + 0.5 * w
        center_y = default_boxes[:, 1] + 0.5 * h

        dx = box_transform_pred[..., 0] / weights[0]
        dy = box_transform_pred[..., 1] / weights[1]
        dw = box_transform_pred[..., 2] / weights[2]
        dh = box_transform_pred[..., 3] / weights[3]

        pred_center_x = dx * w + center_x
        pred_center_y = dy * h + center_y
        pred_w = torch.exp(dw) * w
        pred_h = torch.exp(dh) * h

        pred_box_x1 = pred_center_x - 0.5 * pred_w
        pred_box_y1 = pred_center_y - 0.5 * pred_h
        pred_box_x2 = pred_center_x + 0.5 * pred_w
        pred_box_y2 = pred_center_y + 0.5 * pred_h

        return torch.stack((pred_box_x1, pred_box_y1, pred_box_x2, pred_box_y2), dim=-1)


# Training Function
def train_model():
    # Set random seeds
    torch.manual_seed(config['train_params']['seed'])
    np.random.seed(config['train_params']['seed'])
    random.seed(config['train_params']['seed'])
    if device == 'cuda':
        torch.cuda.manual_seed_all(config['train_params']['seed'])

    # Create datasets - Update normalization for MobileNetV2
    train_dataset = PlantDiseaseDataset('train', config['dataset_params']['train_im_sets'],
                                      config['dataset_params']['im_size'])
    # MobileNetV2 uses different normalization
    train_dataset.imagenet_mean = [0.485, 0.456, 0.406]  # Same as before but ensure it's applied
    train_dataset.imagenet_std = [0.229, 0.224, 0.225]   # Same as before

    train_loader = DataLoader(train_dataset,
                            batch_size=config['train_params']['batch_size'],
                            shuffle=True,
                            collate_fn=lambda x: tuple(zip(*x)),
                            num_workers=4,  # Add workers for better loading
                            pin_memory=True if device == 'cuda' else False)

    # Initialize model
    model = SSD(config['model_params'], config['dataset_params']['num_classes'])
    model.to(device)
    model.train()
    # Print model summary before training
    model.summary()

    # Load checkpoint if exists
    ckpt_path = os.path.join(config['train_params']['task_name'],
                            config['train_params']['ckpt_name'])
    if os.path.exists(ckpt_path):
        print('Loading checkpoint...')
        model.load_state_dict(torch.load(ckpt_path, map_location=device))

    # Create output directory if not exists
    os.makedirs(config['train_params']['task_name'], exist_ok=True)

    # Optimizer - Adjusted for MobileNetV2
    optimizer = torch.optim.SGD([
        {'params': model.features.parameters(), 'lr': config['train_params']['lr'] * 0.01},  # Lower lr for backbone
        {'params': [p for n,p in model.named_parameters() if 'features' not in n], 'lr': config['train_params']['lr']}
    ], momentum=0.9, weight_decay=5e-4)

    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,
                                                   milestones=config['train_params']['lr_steps'],
                                                   gamma=0.5)

    # Training loop with gradient clipping
    num_epochs = config['train_params']['num_epochs']
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        total_cls_loss = 0.0
        total_loc_loss = 0.0
        total_samples = 0

        for batch_idx, (images, targets, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')):
            # Prepare targets
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            # Prepare images - ensure proper normalization
            images = torch.stack([im.float().to(device) for im in images], dim=0)

            # Forward pass
            losses, _ = model(images, targets)

            # Verify loss shapes
            if torch.isnan(losses['classification']) or torch.isnan(losses['bbox_regression']):
                print("NaN detected in losses! Skipping batch...")
                continue

            loss = losses['classification'] + losses['bbox_regression']

            # Backward pass with gradient clipping
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # Gradient clipping
            optimizer.step()

            # Track losses
            total_loss += loss.item() * images.size(0)
            total_cls_loss += losses['classification'].item() * images.size(0)
            total_loc_loss += losses['bbox_regression'].item() * images.size(0)
            total_samples += images.size(0)

            # Log progress
            if (batch_idx + 1) % config['train_params']['log_steps'] == 0:
                avg_loss = total_loss / total_samples
                avg_cls = total_cls_loss / total_samples
                avg_loc = total_loc_loss / total_samples
                print(f'Batch {batch_idx+1}: Total Loss: {avg_loss:.4f}, '
                     f'Cls Loss: {avg_cls:.4f}, Loc Loss: {avg_loc:.4f}')

        # Epoch summary
        avg_loss = total_loss / total_samples
        avg_cls = total_cls_loss / total_samples
        avg_loc = total_loc_loss / total_samples
        print(f'\nEpoch {epoch+1} Summary:')
        print(f'Total Loss: {avg_loss:.4f}')
        print(f'Classification Loss: {avg_cls:.4f}')
        print(f'Localization Loss: {avg_loc:.4f}\n')

        # Update learning rate
        scheduler.step()

        # Save checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss,
        }, ckpt_path)

    print('Training complete!')

# Inference Function
def infer_samples(num_samples=10):
    # Load model
    model = SSD(config['model_params'], config['dataset_params']['num_classes'])
    model.to(device)
    model.eval()

    # Load checkpoint
    ckpt_path = os.path.join(config['train_params']['task_name'], config['train_params']['ckpt_name'])
    if os.path.exists(ckpt_path):
        print('Loading checkpoint...')
        checkpoint = torch.load(ckpt_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        print('No checkpoint found!')
        return

    # Load dataset
    test_dataset = PlantDiseaseDataset('test', config['dataset_params']['test_im_sets'],
                                      config['dataset_params']['im_size'])

    # Create output directory
    os.makedirs('samples', exist_ok=True)

    # Run inference on random samples
    for i in range(num_samples):
        dataset_idx = random.randint(0, len(test_dataset))
        im_tensor, target, fname = test_dataset[dataset_idx]

        # Get ground truth visualization
        gt_im = cv2.imread(fname)
        h, w = gt_im.shape[:2]
        gt_im_copy = gt_im.copy()

        for idx, box in enumerate(target['boxes']):
            x1, y1, x2, y2 = box.detach().cpu().numpy()
            x1, y1, x2, y2 = int(w*x1), int(h*y1), int(w*x2), int(h*y2)
            cv2.rectangle(gt_im, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])
            cv2.rectangle(gt_im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 255, 0])
            text = test_dataset.idx2label[target['labels'][idx].detach().cpu().item()]
            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)
            text_w, text_h = text_size
            cv2.rectangle(gt_im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)
            cv2.putText(gt_im, text=text, org=(x1 + 5, y1 + 15),
                        thickness=1, fontScale=1, color=[0, 0, 0], fontFace=cv2.FONT_HERSHEY_PLAIN)
            cv2.putText(gt_im_copy, text=text, org=(x1 + 5, y1 + 15),
                        thickness=1, fontScale=1, color=[0, 0, 0], fontFace=cv2.FONT_HERSHEY_PLAIN)

        cv2.addWeighted(gt_im_copy, 0.7, gt_im, 0.3, 0, gt_im)
        cv2.imwrite(f'samples/output_ssd_gt_{i}.png', gt_im)

        # Get predictions
        _, detections = model(im_tensor.unsqueeze(0).to(device))


        boxes = detections[0]['boxes']
        labels = detections[0]['labels']
        scores = detections[0]['scores']
        
        indices = torch.argsort(scores, descending=True)
        boxes = boxes[indices]
        labels = labels[indices]
        scores = scores[indices]

        im = cv2.imread(fname)
        im_copy = im.copy()

        for idx, box in enumerate(boxes[0:1]):
            x1, y1, x2, y2 = box.detach().cpu().numpy()
            x1, y1, x2, y2 = int(w * x1), int(h * y1), int(w * x2), int(h * y2)
            cv2.rectangle(im, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])
            cv2.rectangle(im_copy, (x1, y1), (x2, y2), thickness=2, color=[0, 0, 255])
            text = f'{test_dataset.idx2label[labels[idx].item()]}: {scores[idx].item():.2f}'
            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 1, 1)
            text_w, text_h = text_size
            cv2.rectangle(im_copy, (x1, y1), (x1 + 10 + text_w, y1 + 10 + text_h), [255, 255, 255], -1)
            cv2.putText(im, text=text, org=(x1 + 5, y1 + 15),
                        thickness=1, fontScale=1, color=[0, 0, 0], fontFace=cv2.FONT_HERSHEY_PLAIN)
            cv2.putText(im_copy, text=text, org=(x1 + 5, y1 + 15),
                        thickness=1, fontScale=1, color=[0, 0, 0], fontFace=cv2.FONT_HERSHEY_PLAIN)

        cv2.addWeighted(im_copy, 0.7, im, 0.3, 0, im)
        cv2.imwrite(f'samples/output_ssd_{i}.jpg', im)

    print('Inference complete! Check the "samples" directory for results.')


def compute_precision_recall_f1(preds, gts, difficults, iou_threshold=0.5):
    """
    Compute Precision, Recall, and F1 Score for object detection
    """
    # Initialize counters
    true_positives = 0
    false_positives = 0
    false_negatives = 0

    for im_idx in range(len(gts)):
        gt_boxes = gts[im_idx]
        pred_boxes = preds[im_idx]
        difficult = difficults[im_idx]

        # Process each class
        for class_name in gt_boxes.keys():
            # Get all GT boxes for this class (excluding difficult)
            class_gt_boxes = [box for box, is_diff in zip(gt_boxes[class_name], difficult[class_name]) if not is_diff]
            class_pred_boxes = pred_boxes.get(class_name, [])

            # Track matched GT boxes
            gt_matched = [False] * len(class_gt_boxes)

            # Sort predictions by confidence score (descending)
            class_pred_boxes_sorted = sorted(class_pred_boxes, key=lambda x: -x[4])

            # Match predictions to GT
            for pred_box in class_pred_boxes_sorted:
                pred_coords = pred_box[:4]
                max_iou = 0
                best_gt_idx = -1

                for gt_idx, gt_box in enumerate(class_gt_boxes):
                    if gt_matched[gt_idx]:
                        continue

                    iou = calculate_iou(pred_coords, gt_box)
                    if iou > max_iou:
                        max_iou = iou
                        best_gt_idx = gt_idx

                if max_iou >= iou_threshold:
                    gt_matched[best_gt_idx] = True
                    true_positives += 1
                else:
                    false_positives += 1

            # Any unmatched GT boxes are false negatives
            false_negatives += sum(1 for matched in gt_matched if not matched)

    # Calculate metrics
    precision = true_positives / (true_positives + false_positives + 1e-6)
    recall = true_positives / (true_positives + false_negatives + 1e-6)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)

    return precision, recall, f1

def evaluate_map():
    # [Previous evaluation setup code remains the same until after the inference loop...]
     # Load model
    model = SSD(config['model_params'], config['dataset_params']['num_classes'])
    model.to(device)
    model.eval()

    # Load checkpoint
    ckpt_path = os.path.join(config['train_params']['task_name'], config['train_params']['ckpt_name'])
    if os.path.exists(ckpt_path):
        checkpoint = torch.load(ckpt_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        print('No checkpoint found!')
        return

    # Load dataset
    test_dataset = PlantDiseaseDataset('test', config['dataset_params']['test_im_sets'],
                                      config['dataset_params']['im_size'])
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,
                            collate_fn=lambda x: tuple(zip(*x)))

    # Initialize variables for mAP calculation
    gts = []
    preds = []
    difficults = []

    with torch.no_grad():
        for images, targets, _ in tqdm(test_loader, desc='Evaluating'):
            images = torch.stack([im.float().to(device) for im in images], dim=0)
            target = targets[0]

            # Get predictions
            _, detections = model(images)
            detection = detections[0]

            # Prepare ground truth and predictions for mAP calculation
            pred_boxes = {label: [] for label in test_dataset.idx2label.values()}
            gt_boxes = {label: [] for label in test_dataset.idx2label.values()}
            difficult_boxes = {label: [] for label in test_dataset.idx2label.values()}

            # Process predictions
            for box, label, score in zip(detection['boxes'], detection['labels'], detection['scores']):
                label_name = test_dataset.idx2label[label.item()]
                pred_boxes[label_name].append([box[0].item(), box[1].item(),
                                             box[2].item(), box[3].item(), score.item()])

            # Process ground truth
            for box, label, difficult in zip(target['boxes'], target['labels'], target['difficult']):
                label_name = test_dataset.idx2label[label.item()]
                gt_boxes[label_name].append([box[0].item(), box[1].item(),
                                           box[2].item(), box[3].item()])
                difficult_boxes[label_name].append(difficult.item())

            gts.append(gt_boxes)
            preds.append(pred_boxes)
            difficults.append(difficult_boxes)
    # Compute mAP
    mean_ap, class_aps = compute_map(preds, gts, difficults)

    # Compute Precision, Recall, F1 Score
    precision, recall, f1 = compute_precision_recall_f1(preds, gts, difficults)

    print('\nEvaluation Metrics:')
    print('-' * 50)
    print(f"{'Class':<30} | {'AP':<10}")
    print('-' * 50)
    for label, ap in class_aps.items():
        print(f"{label:<30} | {ap:.4f}")
    print('-' * 50)
    print(f"\nMean Average Precision (mAP): {mean_ap:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}\n")

# [Rest of the code remains exactly the same...]
def compute_map(det_boxes, gt_boxes, difficult, iou_threshold=0.5, method='area'):
    gt_labels = {cls_key for im_gt in gt_boxes for cls_key in im_gt.keys()}
    gt_labels = sorted(gt_labels)

    all_aps = {}
    aps = []

    for label in gt_labels:
        cls_dets = [
            [im_idx, im_dets_label]
            for im_idx, im_dets in enumerate(det_boxes)
            if label in im_dets
            for im_dets_label in im_dets[label]
        ]

        cls_dets = sorted(cls_dets, key=lambda k: -k[1][-1])

        gt_matched = [[False for _ in im_gts[label]] for im_gts in gt_boxes]
        num_gts = sum([len(im_gts[label]) for im_gts in gt_boxes])
        num_difficults = sum([sum(difficults_label[label]) for difficults_label in difficult])

        tp = [0] * len(cls_dets)
        fp = [0] * len(cls_dets)

        for det_idx, (im_idx, det_pred) in enumerate(cls_dets):
            im_gts = gt_boxes[im_idx][label]
            im_gt_difficults = difficult[im_idx][label]

            max_iou_found = -1
            max_iou_gt_idx = -1

            for gt_box_idx, gt_box in enumerate(im_gts):
                gt_box_iou = calculate_iou(det_pred[:-1], gt_box)
                if gt_box_iou > max_iou_found:
                    max_iou_found = gt_box_iou
                    max_iou_gt_idx = gt_box_idx

            if max_iou_found >= iou_threshold:
                if not im_gt_difficults[max_iou_gt_idx]:
                    if not gt_matched[im_idx][max_iou_gt_idx]:
                        gt_matched[im_idx][max_iou_gt_idx] = True
                        tp[det_idx] = 1
                    else:
                        fp[det_idx] = 1
            else:
                fp[det_idx] = 1

        tp = np.cumsum(tp)
        fp = np.cumsum(fp)

        eps = np.finfo(np.float32).eps
        recalls = tp / np.maximum(num_gts - num_difficults, eps)
        precisions = tp / np.maximum((tp + fp), eps)

        if method == 'area':
            recalls = np.concatenate(([0.0], recalls, [1.0]))
            precisions = np.concatenate(([0.0], precisions, [0.0]))

            for i in range(precisions.size - 1, 0, -1):
                precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])

            i = np.where(recalls[1:] != recalls[:-1])[0]
            ap = np.sum((recalls[i + 1] - recalls[i]) * precisions[i + 1])
        elif method == 'interp':
            ap = 0.0
            for interp_pt in np.arange(0, 1 + 1E-3, 0.1):
                prec_interp_pt = precisions[recalls >= interp_pt]
                prec_interp_pt = prec_interp_pt.max() if prec_interp_pt.size > 0 else 0.0
                ap += prec_interp_pt
            ap = ap / 11.0
        else:
            raise ValueError('Method can only be area or interp')

        if num_gts > 0:
            aps.append(ap)
            all_aps[label] = ap
        else:
            all_aps[label] = np.nan

    mean_ap = sum(aps) / len(aps) if aps else 0.0
    return mean_ap, all_aps

def calculate_iou(det, gt):
    det_x1, det_y1, det_x2, det_y2 = det
    gt_x1, gt_y1, gt_x2, gt_y2 = gt

    x_left = max(det_x1, gt_x1)
    y_top = max(det_y1, gt_y1)
    x_right = min(det_x2, gt_x2)
    y_bottom = min(det_y2, gt_y2)

    if x_right < x_left or y_bottom < y_top:
        return 0.0

    area_intersection = (x_right - x_left) * (y_bottom - y_top)
    det_area = (det_x2 - det_x1) * (det_y2 - det_y1)
    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
    area_union = float(det_area + gt_area - area_intersection + 1E-6)
    return area_intersection / area_union
if __name__ == '__main__':
    # Example usage:
    train_model()
    #evaluate_map()  # Uncomment to run evaluation
    # To run inference on sample images
    infer_samples(num_samples=5)

