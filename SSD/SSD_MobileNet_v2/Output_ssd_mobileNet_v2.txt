(myvenv) immortal@DESKTOP-0AGQSU3:~$ /home/immortal/myvenv/bin/python /home/immortal/test/ssd_mobilenet_v2.py
Loaded 10326 train images

Model Architecture Summary:
Input shape: torch.Size([1, 3, 300, 300])

MobileNetV2 Backbone:
Sequential(
  (0): Conv2dNormActivation(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (1): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (2): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (3): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (4): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (5): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (6): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (7): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (8): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (9): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (10): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (11): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (12): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (13): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (14): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (15): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (16): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (17): InvertedResidual(
    (conv): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (1): Conv2dNormActivation(
        (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (18): Conv2dNormActivation(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
)

Additional Layers:
Extra Layer 1:
Sequential(
  (0): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1))
  (1): ReLU(inplace=True)
  (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (3): ReLU(inplace=True)
)
Extra Layer 2:
Sequential(
  (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
  (1): ReLU(inplace=True)
  (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (3): ReLU(inplace=True)
)

Prediction Heads:
Classification Heads: ModuleList(
  (0): Conv2d(24, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(32, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): Conv2d(96, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): Conv2d(1280, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): Conv2d(512, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): Conv2d(256, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
Bounding Box Heads: ModuleList(
  (0): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): Conv2d(1280, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

Total Parameters: 9,307,352
Trainable Parameters: 9,307,352

Feature Map Sizes:
Input size: torch.Size([1, 3, 300, 300])
Layer 3 output: torch.Size([1, 24, 75, 75])
Layer 6 output: torch.Size([1, 32, 38, 38])
Layer 13 output: torch.Size([1, 96, 19, 19])
Final features output: torch.Size([1, 1280, 10, 10])
Extra Layer 1 output: torch.Size([1, 512, 5, 5])
Extra Layer 2 output: torch.Size([1, 256, 3, 3])
Epoch 1/32:  31%|████████████████▊                                      | 99/323 [01:54<03:57,  1.06s/it]Batch 100: Total Loss: 29.3347, Cls Loss: 24.8127, Loc Loss: 4.5220
Epoch 1/32:  62%|█████████████████████████████████▎                    | 199/323 [04:00<02:26,  1.18s/it]Batch 200: Total Loss: 25.2831, Cls Loss: 20.9997, Loc Loss: 4.2834
Epoch 1/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [06:03<00:22,  1.09it/s]Batch 300: Total Loss: 23.1743, Cls Loss: 19.0493, Loc Loss: 4.1250
Epoch 1/32: 100%|██████████████████████████████████████████████████████| 323/323 [06:28<00:00,  1.20s/it]

Epoch 1 Summary:
Total Loss: 22.8121
Classification Loss: 18.7226
Localization Loss: 4.0895

Epoch 2/32:  31%|████████████████▊                                      | 99/323 [01:58<04:21,  1.17s/it]Batch 100: Total Loss: 17.3390, Cls Loss: 13.8815, Loc Loss: 3.4575
Epoch 2/32:  62%|█████████████████████████████████▎                    | 199/323 [03:58<02:24,  1.16s/it]Batch 200: Total Loss: 16.8672, Cls Loss: 13.4938, Loc Loss: 3.3734
Epoch 2/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [05:50<00:29,  1.23s/it]Batch 300: Total Loss: 16.4881, Cls Loss: 13.1936, Loc Loss: 3.2945
Epoch 2/32: 100%|██████████████████████████████████████████████████████| 323/323 [06:14<00:00,  1.16s/it]

Epoch 2 Summary:
Total Loss: 16.4054
Classification Loss: 13.1356
Localization Loss: 3.2698

Epoch 3/32:  31%|████████████████▊                                      | 99/323 [01:57<07:12,  1.93s/it]Batch 100: Total Loss: 15.0375, Cls Loss: 11.9823, Loc Loss: 3.0552
Epoch 3/32:  62%|█████████████████████████████████▎                    | 199/323 [03:54<02:53,  1.40s/it]Batch 200: Total Loss: 14.7154, Cls Loss: 11.7105, Loc Loss: 3.0049
Epoch 3/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [05:44<00:23,  1.02it/s]Batch 300: Total Loss: 14.4507, Cls Loss: 11.5069, Loc Loss: 2.9438
Epoch 3/32: 100%|██████████████████████████████████████████████████████| 323/323 [06:09<00:00,  1.14s/it]

Epoch 3 Summary:
Total Loss: 14.3937
Classification Loss: 11.4593
Localization Loss: 2.9344

Epoch 4/32:  31%|████████████████▊                                      | 99/323 [01:57<04:02,  1.08s/it]Batch 100: Total Loss: 13.4596, Cls Loss: 10.6780, Loc Loss: 2.7816
Epoch 4/32:  62%|█████████████████████████████████▎                    | 199/323 [03:53<02:25,  1.17s/it]Batch 200: Total Loss: 13.2239, Cls Loss: 10.4765, Loc Loss: 2.7475
Epoch 4/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [05:46<00:27,  1.13s/it]Batch 300: Total Loss: 13.0002, Cls Loss: 10.2962, Loc Loss: 2.7040
Epoch 4/32: 100%|██████████████████████████████████████████████████████| 323/323 [06:12<00:00,  1.15s/it]

Epoch 4 Summary:
Total Loss: 12.9492
Classification Loss: 10.2544
Localization Loss: 2.6948

Epoch 5/32:  31%|████████████████▊                                      | 99/323 [02:05<03:51,  1.04s/it]Batch 100: Total Loss: 12.1091, Cls Loss: 9.5185, Loc Loss: 2.5906
Epoch 5/32:  62%|█████████████████████████████████▎                    | 199/323 [04:01<02:12,  1.06s/it]Batch 200: Total Loss: 11.9663, Cls Loss: 9.3981, Loc Loss: 2.5681
Epoch 5/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [05:58<00:23,  1.01it/s]Batch 300: Total Loss: 11.8229, Cls Loss: 9.2706, Loc Loss: 2.5523
Epoch 5/32: 100%|██████████████████████████████████████████████████████| 323/323 [06:23<00:00,  1.19s/it]

Epoch 5 Summary:
Total Loss: 11.7983
Classification Loss: 9.2454
Localization Loss: 2.5529

Epoch 6/32:  31%|████████████████▊                                      | 99/323 [02:05<04:27,  1.19s/it]Batch 100: Total Loss: 11.0681, Cls Loss: 8.6065, Loc Loss: 2.4616
Epoch 6/32:  62%|█████████████████████████████████▎                    | 199/323 [04:04<02:25,  1.18s/it]Batch 200: Total Loss: 10.9184, Cls Loss: 8.4785, Loc Loss: 2.4399
Epoch 6/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [06:02<00:27,  1.13s/it]Batch 300: Total Loss: 10.8045, Cls Loss: 8.3787, Loc Loss: 2.4259
Epoch 6/32: 100%|██████████████████████████████████████████████████████| 323/323 [06:35<00:00,  1.22s/it]

Epoch 6 Summary:
Total Loss: 10.7703
Classification Loss: 8.3472
Localization Loss: 2.4231

Epoch 7/32:  31%|████████████████▊                                      | 99/323 [02:20<04:28,  1.20s/it]Batch 100: Total Loss: 10.1843, Cls Loss: 7.8137, Loc Loss: 2.3707
Epoch 7/32:  62%|█████████████████████████████████▎                    | 199/323 [04:19<02:38,  1.28s/it]Batch 200: Total Loss: 10.0872, Cls Loss: 7.7381, Loc Loss: 2.3491
Epoch 7/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [06:20<00:30,  1.27s/it]Batch 300: Total Loss: 9.9814, Cls Loss: 7.6404, Loc Loss: 2.3410
Epoch 7/32: 100%|██████████████████████████████████████████████████████| 323/323 [06:53<00:00,  1.28s/it]

Epoch 7 Summary:
Total Loss: 9.9520
Classification Loss: 7.6111
Localization Loss: 2.3409

Epoch 8/32:  31%|████████████████▊                                      | 99/323 [02:07<04:03,  1.09s/it]Batch 100: Total Loss: 9.4632, Cls Loss: 7.1664, Loc Loss: 2.2968
Epoch 8/32:  62%|█████████████████████████████████▎                    | 199/323 [04:08<02:28,  1.20s/it]Batch 200: Total Loss: 9.3445, Cls Loss: 7.0642, Loc Loss: 2.2802
Epoch 8/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [06:09<00:36,  1.52s/it]Batch 300: Total Loss: 9.2425, Cls Loss: 6.9698, Loc Loss: 2.2727
Epoch 8/32: 100%|██████████████████████████████████████████████████████| 323/323 [06:42<00:00,  1.24s/it]

Epoch 8 Summary:
Total Loss: 9.2174
Classification Loss: 6.9451
Localization Loss: 2.2723

Epoch 9/32:  31%|████████████████▊                                      | 99/323 [02:09<04:56,  1.32s/it]Batch 100: Total Loss: 8.7303, Cls Loss: 6.5256, Loc Loss: 2.2047
Epoch 9/32:  62%|█████████████████████████████████▎                    | 199/323 [04:31<02:59,  1.45s/it]Batch 200: Total Loss: 8.6310, Cls Loss: 6.4358, Loc Loss: 2.1952
Epoch 9/32:  93%|█████████████████████████████████████████████████▉    | 299/323 [06:38<00:28,  1.17s/it]Batch 300: Total Loss: 8.5603, Cls Loss: 6.3644, Loc Loss: 2.1958
Epoch 9/32: 100%|██████████████████████████████████████████████████████| 323/323 [07:05<00:00,  1.32s/it]

Epoch 9 Summary:
Total Loss: 8.5450
Classification Loss: 6.3494
Localization Loss: 2.1956

Epoch 10/32:  31%|████████████████▌                                     | 99/323 [02:03<04:21,  1.17s/it]Batch 100: Total Loss: 8.1707, Cls Loss: 6.0177, Loc Loss: 2.1530
Epoch 10/32:  62%|████████████████████████████████▋                    | 199/323 [04:08<02:57,  1.43s/it]Batch 200: Total Loss: 8.1268, Cls Loss: 5.9751, Loc Loss: 2.1517
Epoch 10/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:13<00:28,  1.17s/it]Batch 300: Total Loss: 8.0399, Cls Loss: 5.8947, Loc Loss: 2.1452
Epoch 10/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:40<00:00,  1.24s/it]

Epoch 10 Summary:
Total Loss: 8.0201
Classification Loss: 5.8769
Localization Loss: 2.1432

Epoch 11/32:  31%|████████████████▌                                     | 99/323 [02:19<04:58,  1.33s/it]Batch 100: Total Loss: 7.6966, Cls Loss: 5.5936, Loc Loss: 2.1030
Epoch 11/32:  62%|████████████████████████████████▋                    | 199/323 [04:23<02:27,  1.19s/it]Batch 200: Total Loss: 7.6337, Cls Loss: 5.5302, Loc Loss: 2.1035
Epoch 11/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:30<00:26,  1.08s/it]Batch 300: Total Loss: 7.5656, Cls Loss: 5.4698, Loc Loss: 2.0959
Epoch 11/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:58<00:00,  1.30s/it]

Epoch 11 Summary:
Total Loss: 7.5593
Classification Loss: 5.4628
Localization Loss: 2.0965

Epoch 12/32:  31%|████████████████▌                                     | 99/323 [02:08<03:58,  1.07s/it]Batch 100: Total Loss: 7.3087, Cls Loss: 5.2296, Loc Loss: 2.0792
Epoch 12/32:  62%|████████████████████████████████▋                    | 199/323 [04:12<02:34,  1.24s/it]Batch 200: Total Loss: 7.2556, Cls Loss: 5.1728, Loc Loss: 2.0827
Epoch 12/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:14<00:31,  1.31s/it]Batch 300: Total Loss: 7.1983, Cls Loss: 5.1320, Loc Loss: 2.0663
Epoch 12/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:41<00:00,  1.24s/it]

Epoch 12 Summary:
Total Loss: 7.1886
Classification Loss: 5.1257
Localization Loss: 2.0628

Epoch 13/32:  31%|████████████████▌                                     | 99/323 [02:07<04:37,  1.24s/it]Batch 100: Total Loss: 7.0285, Cls Loss: 5.0019, Loc Loss: 2.0266
Epoch 13/32:  62%|████████████████████████████████▋                    | 199/323 [04:11<02:23,  1.16s/it]Batch 200: Total Loss: 6.9783, Cls Loss: 4.9588, Loc Loss: 2.0196
Epoch 13/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:12<00:24,  1.01s/it]Batch 300: Total Loss: 6.9093, Cls Loss: 4.8972, Loc Loss: 2.0121
Epoch 13/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:46<00:00,  1.26s/it]

Epoch 13 Summary:
Total Loss: 6.9026
Classification Loss: 4.8893
Localization Loss: 2.0133

Epoch 14/32:  31%|████████████████▌                                     | 99/323 [02:08<04:15,  1.14s/it]Batch 100: Total Loss: 6.7812, Cls Loss: 4.7730, Loc Loss: 2.0082
Epoch 14/32:  62%|████████████████████████████████▋                    | 199/323 [04:06<02:11,  1.06s/it]Batch 200: Total Loss: 6.7369, Cls Loss: 4.7349, Loc Loss: 2.0019
Epoch 14/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:01<00:28,  1.17s/it]Batch 300: Total Loss: 6.6984, Cls Loss: 4.7016, Loc Loss: 1.9968
Epoch 14/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:31<00:00,  1.21s/it]

Epoch 14 Summary:
Total Loss: 6.6861
Classification Loss: 4.6926
Localization Loss: 1.9934

Epoch 15/32:  31%|████████████████▌                                     | 99/323 [02:15<04:34,  1.22s/it]Batch 100: Total Loss: 6.5749, Cls Loss: 4.5771, Loc Loss: 1.9978
Epoch 15/32:  62%|████████████████████████████████▋                    | 199/323 [04:11<02:14,  1.09s/it]Batch 200: Total Loss: 6.5216, Cls Loss: 4.5507, Loc Loss: 1.9709
Epoch 15/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:01<00:26,  1.10s/it]Batch 300: Total Loss: 6.4973, Cls Loss: 4.5335, Loc Loss: 1.9638
Epoch 15/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:32<00:00,  1.22s/it]

Epoch 15 Summary:
Total Loss: 6.4899
Classification Loss: 4.5271
Localization Loss: 1.9628

Epoch 16/32:  31%|████████████████▌                                     | 99/323 [02:21<06:50,  1.83s/it]Batch 100: Total Loss: 6.3150, Cls Loss: 4.3908, Loc Loss: 1.9242
Epoch 16/32:  62%|████████████████████████████████▋                    | 199/323 [04:18<01:59,  1.04it/s]Batch 200: Total Loss: 6.3273, Cls Loss: 4.4055, Loc Loss: 1.9217
Epoch 16/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:11<00:25,  1.07s/it]Batch 300: Total Loss: 6.3170, Cls Loss: 4.3920, Loc Loss: 1.9250
Epoch 16/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:37<00:00,  1.23s/it]

Epoch 16 Summary:
Total Loss: 6.3151
Classification Loss: 4.3894
Localization Loss: 1.9257

Epoch 17/32:  31%|████████████████▌                                     | 99/323 [02:03<04:58,  1.33s/it]Batch 100: Total Loss: 6.2918, Cls Loss: 4.3723, Loc Loss: 1.9195
Epoch 17/32:  62%|████████████████████████████████▋                    | 199/323 [04:14<02:35,  1.25s/it]Batch 200: Total Loss: 6.2386, Cls Loss: 4.3312, Loc Loss: 1.9074
Epoch 17/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:08<00:25,  1.07s/it]Batch 300: Total Loss: 6.2119, Cls Loss: 4.3055, Loc Loss: 1.9064
Epoch 17/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:34<00:00,  1.22s/it]

Epoch 17 Summary:
Total Loss: 6.2060
Classification Loss: 4.3011
Localization Loss: 1.9049

Epoch 18/32:  31%|████████████████▌                                     | 99/323 [02:15<03:50,  1.03s/it]Batch 100: Total Loss: 6.1144, Cls Loss: 4.2290, Loc Loss: 1.8855
Epoch 18/32:  62%|████████████████████████████████▋                    | 199/323 [04:08<02:27,  1.19s/it]Batch 200: Total Loss: 6.0981, Cls Loss: 4.2195, Loc Loss: 1.8786
Epoch 18/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:57<00:27,  1.16s/it]Batch 300: Total Loss: 6.0702, Cls Loss: 4.2018, Loc Loss: 1.8684
Epoch 18/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:21<00:00,  1.18s/it]

Epoch 18 Summary:
Total Loss: 6.0705
Classification Loss: 4.1987
Localization Loss: 1.8718

Epoch 19/32:  31%|████████████████▌                                     | 99/323 [02:10<03:54,  1.05s/it]Batch 100: Total Loss: 6.0476, Cls Loss: 4.1705, Loc Loss: 1.8771
Epoch 19/32:  62%|████████████████████████████████▋                    | 199/323 [04:00<02:21,  1.14s/it]Batch 200: Total Loss: 6.0141, Cls Loss: 4.1585, Loc Loss: 1.8556
Epoch 19/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:51<00:22,  1.05it/s]Batch 300: Total Loss: 5.9981, Cls Loss: 4.1421, Loc Loss: 1.8560
Epoch 19/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:15<00:00,  1.16s/it]

Epoch 19 Summary:
Total Loss: 5.9973
Classification Loss: 4.1402
Localization Loss: 1.8572

Epoch 20/32:  31%|████████████████▌                                     | 99/323 [01:53<03:44,  1.00s/it]Batch 100: Total Loss: 5.9225, Cls Loss: 4.0846, Loc Loss: 1.8378
Epoch 20/32:  62%|████████████████████████████████▋                    | 199/323 [03:46<02:12,  1.07s/it]Batch 200: Total Loss: 5.9271, Cls Loss: 4.0883, Loc Loss: 1.8388
Epoch 20/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:37<00:25,  1.08s/it]Batch 300: Total Loss: 5.9247, Cls Loss: 4.0894, Loc Loss: 1.8352
Epoch 20/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:06<00:00,  1.14s/it]

Epoch 20 Summary:
Total Loss: 5.9210
Classification Loss: 4.0861
Localization Loss: 1.8349

Epoch 21/32:  31%|████████████████▌                                     | 99/323 [02:00<03:38,  1.03it/s]Batch 100: Total Loss: 5.8969, Cls Loss: 4.0701, Loc Loss: 1.8267
Epoch 21/32:  62%|████████████████████████████████▋                    | 199/323 [03:51<02:07,  1.03s/it]Batch 200: Total Loss: 5.8554, Cls Loss: 4.0418, Loc Loss: 1.8136
Epoch 21/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:51<00:31,  1.33s/it]Batch 300: Total Loss: 5.8553, Cls Loss: 4.0360, Loc Loss: 1.8193
Epoch 21/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:14<00:00,  1.16s/it]

Epoch 21 Summary:
Total Loss: 5.8532
Classification Loss: 4.0368
Localization Loss: 1.8164

Epoch 22/32:  31%|████████████████▌                                     | 99/323 [02:09<04:17,  1.15s/it]Batch 100: Total Loss: 5.8295, Cls Loss: 4.0240, Loc Loss: 1.8055
Epoch 22/32:  62%|████████████████████████████████▋                    | 199/323 [04:02<01:40,  1.23it/s]Batch 200: Total Loss: 5.8544, Cls Loss: 4.0402, Loc Loss: 1.8142
Epoch 22/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:05<00:20,  1.20it/s]Batch 300: Total Loss: 5.8335, Cls Loss: 4.0262, Loc Loss: 1.8073
Epoch 22/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:48<00:00,  1.27s/it]

Epoch 22 Summary:
Total Loss: 5.8267
Classification Loss: 4.0230
Localization Loss: 1.8036

Epoch 23/32:  31%|████████████████▌                                     | 99/323 [02:17<09:41,  2.59s/it]Batch 100: Total Loss: 5.7484, Cls Loss: 3.9583, Loc Loss: 1.7901
Epoch 23/32:  62%|████████████████████████████████▋                    | 199/323 [04:29<02:19,  1.12s/it]Batch 200: Total Loss: 5.7739, Cls Loss: 3.9805, Loc Loss: 1.7933
Epoch 23/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:23<00:24,  1.04s/it]Batch 300: Total Loss: 5.7650, Cls Loss: 3.9738, Loc Loss: 1.7912
Epoch 23/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:53<00:00,  1.28s/it]

Epoch 23 Summary:
Total Loss: 5.7684
Classification Loss: 3.9751
Localization Loss: 1.7933

Epoch 24/32:  31%|████████████████▌                                     | 99/323 [02:26<03:54,  1.05s/it]Batch 100: Total Loss: 5.7089, Cls Loss: 3.9384, Loc Loss: 1.7705
Epoch 24/32:  62%|████████████████████████████████▋                    | 199/323 [04:33<01:52,  1.10it/s]Batch 200: Total Loss: 5.7034, Cls Loss: 3.9347, Loc Loss: 1.7687
Epoch 24/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:47<00:24,  1.01s/it]Batch 300: Total Loss: 5.7100, Cls Loss: 3.9354, Loc Loss: 1.7746
Epoch 24/32: 100%|█████████████████████████████████████████████████████| 323/323 [07:13<00:00,  1.34s/it]

Epoch 24 Summary:
Total Loss: 5.7173
Classification Loss: 3.9402
Localization Loss: 1.7771

Epoch 25/32:  31%|████████████████▌                                     | 99/323 [02:29<04:22,  1.17s/it]Batch 100: Total Loss: 5.6620, Cls Loss: 3.9126, Loc Loss: 1.7494
Epoch 25/32:  62%|████████████████████████████████▋                    | 199/323 [04:24<03:01,  1.46s/it]Batch 200: Total Loss: 5.6551, Cls Loss: 3.9006, Loc Loss: 1.7545
Epoch 25/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:16<00:26,  1.09s/it]Batch 300: Total Loss: 5.6391, Cls Loss: 3.8980, Loc Loss: 1.7411
Epoch 25/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:47<00:00,  1.26s/it]

Epoch 25 Summary:
Total Loss: 5.6390
Classification Loss: 3.9000
Localization Loss: 1.7390

Epoch 26/32:  31%|████████████████▌                                     | 99/323 [01:55<03:28,  1.08it/s]Batch 100: Total Loss: 5.6279, Cls Loss: 3.8878, Loc Loss: 1.7400
Epoch 26/32:  62%|████████████████████████████████▋                    | 199/323 [03:46<02:27,  1.19s/it]Batch 200: Total Loss: 5.6349, Cls Loss: 3.8879, Loc Loss: 1.7470
Epoch 26/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:39<00:26,  1.11s/it]Batch 300: Total Loss: 5.6305, Cls Loss: 3.8832, Loc Loss: 1.7474
Epoch 26/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:04<00:00,  1.13s/it]

Epoch 26 Summary:
Total Loss: 5.6329
Classification Loss: 3.8861
Localization Loss: 1.7468

Epoch 27/32:  31%|████████████████▌                                     | 99/323 [01:53<03:49,  1.02s/it]Batch 100: Total Loss: 5.5671, Cls Loss: 3.8331, Loc Loss: 1.7340
Epoch 27/32:  62%|████████████████████████████████▋                    | 199/323 [03:46<02:19,  1.13s/it]Batch 200: Total Loss: 5.5843, Cls Loss: 3.8562, Loc Loss: 1.7281
Epoch 27/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:39<00:26,  1.08s/it]Batch 300: Total Loss: 5.5996, Cls Loss: 3.8620, Loc Loss: 1.7376
Epoch 27/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:03<00:00,  1.13s/it]

Epoch 27 Summary:
Total Loss: 5.5982
Classification Loss: 3.8617
Localization Loss: 1.7366

Epoch 28/32:  31%|████████████████▌                                     | 99/323 [02:16<04:35,  1.23s/it]Batch 100: Total Loss: 5.5715, Cls Loss: 3.8204, Loc Loss: 1.7511
Epoch 28/32:  62%|████████████████████████████████▋                    | 199/323 [04:23<02:23,  1.16s/it]Batch 200: Total Loss: 5.5782, Cls Loss: 3.8396, Loc Loss: 1.7387
Epoch 28/32:  93%|█████████████████████████████████████████████████    | 299/323 [06:15<00:25,  1.05s/it]Batch 300: Total Loss: 5.5699, Cls Loss: 3.8408, Loc Loss: 1.7290
Epoch 28/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:45<00:00,  1.26s/it]

Epoch 28 Summary:
Total Loss: 5.5645
Classification Loss: 3.8383
Localization Loss: 1.7262

Epoch 29/32:  31%|████████████████▌                                     | 99/323 [02:11<03:45,  1.01s/it]Batch 100: Total Loss: 5.5394, Cls Loss: 3.8196, Loc Loss: 1.7197
Epoch 29/32:  62%|████████████████████████████████▋                    | 199/323 [04:06<02:06,  1.02s/it]Batch 200: Total Loss: 5.5387, Cls Loss: 3.8211, Loc Loss: 1.7176
Epoch 29/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:59<00:25,  1.05s/it]Batch 300: Total Loss: 5.5488, Cls Loss: 3.8302, Loc Loss: 1.7186
Epoch 29/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:24<00:00,  1.19s/it]

Epoch 29 Summary:
Total Loss: 5.5405
Classification Loss: 3.8263
Localization Loss: 1.7143

Epoch 30/32:  31%|████████████████▌                                     | 99/323 [01:55<04:21,  1.17s/it]Batch 100: Total Loss: 5.5477, Cls Loss: 3.8285, Loc Loss: 1.7193
Epoch 30/32:  62%|████████████████████████████████▋                    | 199/323 [03:45<02:30,  1.22s/it]Batch 200: Total Loss: 5.5218, Cls Loss: 3.8074, Loc Loss: 1.7144
Epoch 30/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:37<00:24,  1.04s/it]Batch 300: Total Loss: 5.5107, Cls Loss: 3.8081, Loc Loss: 1.7026
Epoch 30/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:03<00:00,  1.12s/it]

Epoch 30 Summary:
Total Loss: 5.5045
Classification Loss: 3.8047
Localization Loss: 1.6998

Epoch 31/32:  31%|████████████████▌                                     | 99/323 [01:56<03:52,  1.04s/it]Batch 100: Total Loss: 5.5091, Cls Loss: 3.7976, Loc Loss: 1.7114
Epoch 31/32:  62%|████████████████████████████████▋                    | 199/323 [03:47<01:47,  1.15it/s]Batch 200: Total Loss: 5.5178, Cls Loss: 3.8097, Loc Loss: 1.7080
Epoch 31/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:41<00:26,  1.11s/it]Batch 300: Total Loss: 5.5072, Cls Loss: 3.7980, Loc Loss: 1.7092
Epoch 31/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:05<00:00,  1.13s/it]

Epoch 31 Summary:
Total Loss: 5.4964
Classification Loss: 3.7922
Localization Loss: 1.7042

Epoch 32/32:  31%|████████████████▌                                     | 99/323 [01:56<04:12,  1.13s/it]Batch 100: Total Loss: 5.4973, Cls Loss: 3.7948, Loc Loss: 1.7025
Epoch 32/32:  62%|████████████████████████████████▋                    | 199/323 [03:49<02:19,  1.12s/it]Batch 200: Total Loss: 5.4930, Cls Loss: 3.7917, Loc Loss: 1.7013
Epoch 32/32:  93%|█████████████████████████████████████████████████    | 299/323 [05:40<00:25,  1.05s/it]Batch 300: Total Loss: 5.4689, Cls Loss: 3.7795, Loc Loss: 1.6894
Epoch 32/32: 100%|█████████████████████████████████████████████████████| 323/323 [06:07<00:00,  1.14s/it]

Epoch 32 Summary:
Total Loss: 5.4047
Classification Loss: 3.7334
Localization Loss: 1.6713

Training complete!
Loaded 497 test images
Evaluating: 100%|██████████████████████████████████████████████████████| 497/497 [01:04<00:00,  7.73it/s]

Evaluation Metrics:
--------------------------------------------------
Class                          | AP        
--------------------------------------------------
Cassava Bacterial Blight       | 0.0000
Cassava Brown Leaf Spot        | 0.1797
Cassava Healthy                | 0.5874
Cassava Mosaic                 | 0.5799
Cassava Root Rot               | 0.3750
Corn Brown Spots               | 0.3793
Corn Charcoal                  | 0.0000
Corn Chlorotic Leaf Spot       | 0.5000
Corn Gray leaf spot            | 1.0000
Corn Healthy                   | 0.1233
Corn Insects Damages           | 0.5000
Corn Mildew                    | 0.0000
Corn Purple Discoloration      | nan
Corn Smut                      | nan
Corn Streak                    | 0.3884
Corn Stripe                    | 0.6976
Corn Violet Decoloration       | 0.0000
Corn Yellow Spots              | nan
Corn Yellowing                 | 0.4297
Corn leaf blight               | 0.5785
Corn rust leaf                 | 0.6735
Tomato Brown Spots             | 0.5267
Tomato bacterial wilt          | nan
Tomato blight leaf             | 0.3527
Tomato healthy                 | 0.0000
Tomato leaf mosaic virus       | 0.0000
Tomato leaf yellow virus       | 0.3007
background                     | nan
--------------------------------------------------

Mean Average Precision (mAP): 0.3553
Precision: 0.0404
Recall: 0.6771
F1 Score: 0.0762

Loading checkpoint...
Loaded 497 test images
Inference complete! Check the "samples" directory for results.